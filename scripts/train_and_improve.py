"""
Script complet pour entra√Æner et am√©liorer les mod√®les de pr√©diction de draft LoL.
Teste et compare plusieurs algorithmes avec validation crois√©e et optimisation d'hyperparam√®tres.
"""

import os 
import sys
import argparse
import pickle
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# Imports scikit-learn
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.model_selection import cross_validate, GridSearchCV, train_test_split, StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

# Imports gradient boosting (optionnels mais recommand√©s)
try:
    from lightgbm import LGBMClassifier
    HAS_LIGHTGBM = True
except ImportError:
    HAS_LIGHTGBM = False
    print("‚ö†Ô∏è  LightGBM non install√©. Installer avec: pip install lightgbm")

try:
    from xgboost import XGBClassifier
    HAS_XGBOOST = True
except ImportError:
    HAS_XGBOOST = False
    print("‚ö†Ô∏è  XGBoost non install√©. Installer avec: pip install xgboost")


class ModelComparator:
    """
    Compare plusieurs mod√®les ML pour pr√©diction de draft LoL.
    """
    
    def __init__(self, df, model_dir="../models/saved_models"):
        self.df = df
        self.model_dir = model_dir
        self.results = {}
        self.best_models = {}
        
    def prepare_data(self, target_col, feature_cols):
        """
        Pr√©pare les donn√©es pour l'entra√Ænement.
        
        Returns:
            X_encoded, y_encoded, encoder, valid_indices, label_encoder
        """
        # Filtre les lignes avec donn√©es manquantes
        df_f = self.df.dropna(subset=feature_cols + [target_col])
        if df_f.empty:
            return None, None, None, None
        
        X = df_f[feature_cols]
        y = df_f[target_col]
        
        # Garde uniquement les classes apparaissant au moins 2 fois
        freq = y.value_counts()
        valid = freq[freq >= 2].index
        mask = y.isin(valid)
        X = X[mask]
        y = y[mask]
        
        if X.shape[0] < 10 or y.nunique() < 2:
            return None, None, None, None
        
        # One-hot encode
        encoder = OneHotEncoder(sparse_output=False, handle_unknown="ignore")
        X_encoded = encoder.fit_transform(X)

        # Encode labels as integers (required by XGBoost >=2.0)
        label_encoder = LabelEncoder()
        y_encoded = label_encoder.fit_transform(y)

        return X_encoded, y_encoded, encoder, df_f[mask].index, label_encoder
    
    def evaluate_model(self, model_name, model, X_train, X_test, y_train, y_test):
        """
        √âvalue un mod√®le et retourne les m√©triques.
        """
        try:
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            
            # Calcul des m√©triques
            accuracy = accuracy_score(y_test, y_pred)
            f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)
            precision = precision_score(y_test, y_pred, average='macro', zero_division=0)
            recall = recall_score(y_test, y_pred, average='macro', zero_division=0)
            
            return {
                'accuracy': accuracy,
                'f1_macro': f1_macro,
                'precision': precision,
                'recall': recall,
                'model': model
            }
        except Exception as e:
            print(f"  ‚ùå Erreur avec {model_name}: {e}")
            return None
    
    def cross_validate_model(self, model_name, model, X, y, cv=5):
        """
        Validation crois√©e sur un mod√®le.
        """
        try:
            scoring = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro']
            # R√©duit n_jobs pour √©viter les probl√®mes m√©moire
            cv_results = cross_validate(model, X, y, cv=cv, scoring=scoring, 
                                       return_train_score=False, n_jobs=1)
            
            return {
                'accuracy_mean': cv_results['test_accuracy'].mean(),
                'accuracy_std': cv_results['test_accuracy'].std(),
                'f1_macro_mean': cv_results['test_f1_macro'].mean(),
                'f1_macro_std': cv_results['test_f1_macro'].std(),
                'precision_mean': cv_results['test_precision_macro'].mean(),
                'recall_mean': cv_results['test_recall_macro'].mean(),
            }
        except Exception as e:
            print(f"  ‚ùå Erreur CV avec {model_name}: {e}")
            return None
    
    def optimize_hyperparameters(self, model_name, model, X_train, y_train, param_grid, cv=5):
        """
        Optimise les hyperparam√®tres avec GridSearchCV.
        """
        try:
            grid_search = GridSearchCV(model, param_grid, cv=cv, n_jobs=-1, 
                                      scoring='f1_macro', verbose=0)
            grid_search.fit(X_train, y_train)
            
            return {
                'best_params': grid_search.best_params_,
                'best_score': grid_search.best_score_,
                'best_model': grid_search.best_estimator_
            }
        except Exception as e:
            print(f"  ‚ùå Erreur tuning avec {model_name}: {e}")
            return None
    
    def compare_models_for_target(self, target_col, feature_cols, verbose=True):
        """
        Compare tous les mod√®les pour une cible donn√©e.
        """
        if verbose:
            print(f"\n{'='*70}")
            print(f"üéØ TARGET: {target_col}")
            print(f"{'='*70}")
        
        # Pr√©pare les donn√©es
        X_encoded, y_encoded, encoder, valid_idx, label_encoder = self.prepare_data(target_col, feature_cols)
        if X_encoded is None:
            if verbose:
                print(f"  ‚ö†Ô∏è  Donn√©es insuffisantes pour {target_col}")
            return None

        # Split train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X_encoded, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
        )
        
        if verbose:
            print(f"  üìä Samples: {len(X_encoded)} | Train: {len(X_train)} | Test: {len(X_test)}")
            print(f"  üè∑Ô∏è  Classes: {len(label_encoder.classes_)}")
        
        results_for_target = {}
        
        # Mod√®les √† tester
        models = {
            'LinearSVC (baseline)': LinearSVC(C=1.0, max_iter=5000, dual=True, random_state=42),
            'LinearSVC (C=10)': LinearSVC(C=10.0, max_iter=5000, dual=True, random_state=42),
            'RandomForest': RandomForestClassifier(
                n_estimators=50,         # R√©duit de 100 √† 50
                max_depth=20,            # Limite la profondeur des arbres
                min_samples_split=10,    # Augmente le seuil de split
                min_samples_leaf=5,      # Augmente le nombre min de samples par feuille
                max_features='sqrt',     # R√©duit les features consid√©r√©es
                n_jobs=2,                # Limite le parall√©lisme pour √©conomiser m√©moire
                random_state=42
            ),
        }
        
        if HAS_LIGHTGBM:
            models['LightGBM'] = LGBMClassifier(n_estimators=100, random_state=42, verbose=-1)
        
        if HAS_XGBOOST:
            models['XGBoost'] = XGBClassifier(n_estimators=100, random_state=42, verbose=0)
        
        # √âvaluation simple
        if verbose:
            print(f"\nüìà Test Set Performance (80/20 split):")
        
        for model_name, model in models.items():
            metrics = self.evaluate_model(model_name, model, X_train, X_test, y_train, y_test)
            
            if metrics:
                results_for_target[model_name] = metrics
                if verbose:
                    print(f"  {model_name:25} | "
                          f"Acc: {metrics['accuracy']:.3f} | "
                          f"F1: {metrics['f1_macro']:.3f} | "
                          f"Prec: {metrics['precision']:.3f} | "
                          f"Rec: {metrics['recall']:.3f}")
        
        # Validation crois√©e pour le meilleur mod√®le
        if results_for_target:
            best_model_name = max(results_for_target.keys(), 
                                 key=lambda x: results_for_target[x]['f1_macro'])
            best_model = results_for_target[best_model_name]['model']
            
            if verbose:
                print(f"\nüîÑ Cross-Validation (5-fold) pour: {best_model_name}")
            
            cv_results = self.cross_validate_model(best_model_name, best_model, X_encoded, y_encoded, cv=5)
            
            if cv_results and verbose:
                print(f"  Accuracy: {cv_results['accuracy_mean']:.3f} ¬± {cv_results['accuracy_std']:.3f}")
                print(f"  F1-Macro: {cv_results['f1_macro_mean']:.3f} ¬± {cv_results['f1_macro_std']:.3f}")
            
            results_for_target[best_model_name]['cv_results'] = cv_results
        
        return results_for_target
    
    def run_full_evaluation(self, num_targets=5):
        """
        √âvalue les mod√®les sur les principales cibles.
        """
        print("\n" + "="*70)
        print("üöÄ COMPARAISON COMPL√àTE DES MOD√àLES")
        print("="*70)
        
        # Cibles principales
        configs = [
            ("rb1", ["bb1"]),
            ("bb2", ["bb1", "rb1"]),
            ("rb2", ["bb1", "rb1", "bb2"]),
            ("bp1", ["bb1", "rb1", "bb2", "rb2", "bb3", "rb3"]),
            ("rp1", ["bb1", "rb1", "bb2", "rb2", "bb3", "rb3", "bp1"]),
        ]
        
        for i, (target, features) in enumerate(configs[:num_targets]):
            result = self.compare_models_for_target(target, features, verbose=True)
            self.results[target] = result
        
        # R√©sum√© final
        self.print_summary()
    
    def print_summary(self):
        """
        Affiche un r√©sum√© des r√©sultats.
        """
        print("\n" + "="*70)
        print("üìã R√âSUM√â FINAL")
        print("="*70)
        
        for target, models_results in self.results.items():
            if models_results:
                best_model = max(models_results.keys(), 
                               key=lambda x: models_results[x]['f1_macro'])
                best_f1 = models_results[best_model]['f1_macro']
                print(f"{target}: {best_model} (F1: {best_f1:.3f})")
    
    def save_best_models(self, output_dir="../models/improved_models"):
        """
        Sauvegarde les meilleurs mod√®les.
        """
        os.makedirs(output_dir, exist_ok=True)
        
        for target, models_results in self.results.items():
            if models_results:
                best_model_name = max(models_results.keys(), 
                                     key=lambda x: models_results[x]['f1_macro'])
                best_model = models_results[best_model_name]['model']
                
                filepath = os.path.join(output_dir, f"{target}_model.pkl")
                with open(filepath, 'wb') as f:
                    pickle.dump(best_model, f)
        
        print(f"\n‚úÖ Mod√®les sauvegard√©s dans: {output_dir}")


def main():
    """
    Fonction principale.
    """
    parser = argparse.ArgumentParser(description="Entra√Ænement et am√©lioration des mod√®les LoL")
    parser.add_argument(
        "--csv",
        type=str,
        default="../processed_data/csv_games_fusionnes.csv",
        help="Chemin vers le fichier csv_games_fusionnes.csv"
    )
    args = parser.parse_args()
    
    # Charge les donn√©es
    print("üì• Chargement des donn√©es...")
    print(f"üìÅ Fichier: {args.csv}")
    
    if not os.path.exists(args.csv):
        print(f"‚ùå Erreur: Le fichier {args.csv} n'existe pas")
        sys.exit(1)
    
    df = pd.read_csv(args.csv)
    print(f"‚úÖ Donn√©es charg√©es: {df.shape[0]} lignes, {df.shape[1]} colonnes")
    
    # Lance la comparaison
    comparator = ModelComparator(df)
    comparator.run_full_evaluation(num_targets=5)
    
    # Sauvegarde les meilleurs mod√®les
    comparator.save_best_models()
    
    print("\n" + "="*70)
    print("‚ú® Script d'am√©lioration termin√©!")
    print("="*70)


if __name__ == "__main__":
    main()
